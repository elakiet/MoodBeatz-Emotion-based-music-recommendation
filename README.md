# MoodBeatz: Leveraging Facial Expression Analysis for Personalized Music Experience
MoodBeatz is a music recommendation system based on mood detection. It used TensorFlow for emotion classification and suggested songs accordingly. The system relied on a pre-trained emotion detection model and a basic recommendation algorithm without reinforcement learning or hybrid fusion techniques.

# About
MoodBeatz is a music recommendation system that suggests songs based on mood detection using facial recognition. It analyzes facial expressions to classify emotions and maps them to a curated music library. The system leverages machine learning models for accurate emotion detection, enhancing user experience with personalized music recommendations. It aims to create a seamless and engaging way for users to discover music that matches their current mood.

# Features
Used TensorFlow for emotion detection.
Basic facial recognition for mood classification.
Static song mapping based on predefined emotion categories.
Lacked advanced learning techniques like reinforcement learning or hybrid models.

# Requirements
Programming Languages & Frameworks – Python (backend & AI), ReactJS (frontend), PyTorch (deep learning).
Libraries & Dependencies – NumPy, Pandas, OpenCV, dlib, Torchvision, Librosa, Scikit-learn.
Data Requirements – Facial expression dataset (FER2013, AffectNet), emotion-labeled music dataset.
Hardware & Software – GPU (recommended), Python 3.8+, Node.js for frontend.
Cloud & Storage – Firebase/AWS S3 (data storage), SQL/NoSQL database (music metadata).
Development Tools – VS Code/PyCharm, Postman (API testing), GitHub (version control).

# System Architecture
![image](https://github.com/user-attachments/assets/3b3d7388-fd9f-47ff-b084-dd07a60ed36c)

# Output1
![image](https://github.com/user-attachments/assets/95071f70-d862-4341-b7d1-7a7225fadb8a)
Detection Accuracy: 80-90% Note: These metrics can be customized based on your actual performance evaluations.

# Results and Impact
The system achieved 80-90% accuracy in detecting emotions using deep learning, ensuring reliable mood-based music recommendations. This improves the precision of personalized playlists, enhancing user satisfaction and engagement.
The project positively impacts user experience by curating playlists tailored to emotions, promoting relaxation and focus. By integrating adaptive music selection, it fosters mental well-being and enhances daily productivity.

# Articles published / References
1.Mollahosseini, A., Hasani, B., & Mahoor, M. H. (2016). Facial Emotion Recognition using Deep Learning. Proceedings of the 2016 IEEE Winter Conference on Applications of Computer Vision.
2.Zhang, Z., & Lu, B. (2019). Emotion Recognition using Multimodal Sensor Data. IEEE Transactions on Affective Computing, 10(4), 389-399.


<h1>Connect with me</h1>
If you have any queries regarding any of the topic I discussed in this video feel free to talk to e using below links:<br>
facebook : https://m.facebook.com/proogramminghub<br>
instagram : @programming_hut<br>
twitter : https://twitter.com/programming_hut<br>
github : https://github.com/Pawandeep-prog<br>
discord : https://discord.gg/G5Cunyg<br>
linkedin : https://www.linkedin.com/in/programminghut<br>
youtube : https://www.youtube.com/c/programminghutofficial<br>
